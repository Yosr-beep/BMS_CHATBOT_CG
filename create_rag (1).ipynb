{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9764f2-f4af-49ce-a527-6c4f745c111f",
   "metadata": {
    "id": "5a9764f2-f4af-49ce-a527-6c4f745c111f"
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb284ab3-c42a-4787-815c-62bb4d7d179f",
   "metadata": {
    "id": "eb284ab3-c42a-4787-815c-62bb4d7d179f"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "# from langchain_community.document_loaders import Docx2txtLoader\n",
    "import os\n",
    "import json\n",
    "\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "import requests\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33955a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "folder_path = \"Inputs GenAI BMS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M5xpfeHD6Dzk",
   "metadata": {
    "id": "M5xpfeHD6Dzk"
   },
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821726e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88df440c-7f82-489b-a445-fe0aca9a072c",
   "metadata": {
    "id": "88df440c-7f82-489b-a445-fe0aca9a072c",
    "outputId": "7f37ccaa-87c4-4b20-ac2e-7773b88234da"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber  # Extraction texte des PDFs\n",
    "import pytesseract  # OCR pour images et PDFs scann√©s\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from pdf2image import convert_from_path  # Convertir PDF en images\n",
    "from pptx import Presentation  # Extraction texte PowerPoint\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "folder_path = \"Inputs GenAI BMS\"  # Modifier avec votre chemin r√©el\n",
    "output_csv = \"extracted_dataset.csv\"\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"/usr/bin/tesseract\"  # Modifier selon installation\n",
    "\n",
    "# Liste des donn√©es extraites\n",
    "dataset = []\n",
    "\n",
    "# V√©rifier si le dossier existe\n",
    "if not os.path.exists(folder_path):\n",
    "    print(f\"‚ùå Dossier introuvable : {folder_path}\")\n",
    "    exit()\n",
    "\n",
    "# V√©rifier le nombre de fichiers trouv√©s\n",
    "files = os.listdir(folder_path)\n",
    "if not files:\n",
    "    print(\"‚ùå Aucun fichier trouv√© dans le dossier.\")\n",
    "    exit()\n",
    "print(f\"üìÇ {len(files)} fichiers trouv√©s dans {folder_path}\")\n",
    "\n",
    "# Fonction pour extraire du texte depuis un PDF avec pdfplumber\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text_pages = []\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                text = page.extract_text() or \"\"  # R√©cup√©rer le texte ou une cha√Æne vide\n",
    "                text_pages.append({\"page_num\": page_num, \"text\": text.strip()})\n",
    "        return text_pages\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur extraction PDF ({pdf_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# Fonction OCR sur images d'un PDF\n",
    "def extract_ocr_from_pdf(pdf_path, dpi=150, max_pages=5):\n",
    "    ocr_text = []\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=max_pages)\n",
    "        for img_num, img in enumerate(images, start=1):\n",
    "            img_array = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "            h, w, _ = img_array.shape\n",
    "            if w < 50 or h < 50:\n",
    "                continue  # Ignorer les petites images\n",
    "\n",
    "            gray = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "            text = pytesseract.image_to_string(gray, lang=\"fra+eng\").strip()\n",
    "            if text:\n",
    "                ocr_text.append({\"page_num\": img_num, \"ocr_text\": text})\n",
    "        return ocr_text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur OCR PDF ({pdf_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# Fonction pour extraire le texte d'un PowerPoint\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    text = []\n",
    "    try:\n",
    "        prs = Presentation(pptx_path)\n",
    "        for slide_num, slide in enumerate(prs.slides, start=1):\n",
    "            slide_text = \"\\n\".join(shape.text.strip() for shape in slide.shapes if hasattr(shape, \"text\"))\n",
    "            text.append({\"slide_num\": slide_num, \"text\": slide_text.strip()})\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur extraction PPTX ({pptx_path}): {e}\")\n",
    "        return []\n",
    "\n",
    "# Traitement des fichiers\n",
    "df_data = []\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        print(f\"üìÑ Extraction PDF : {file_name}\")\n",
    "        pdf_text = extract_text_from_pdf(file_path)\n",
    "        pdf_ocr = extract_ocr_from_pdf(file_path, dpi=150, max_pages=5)\n",
    "\n",
    "        for page in pdf_text:\n",
    "            page_num = page[\"page_num\"]\n",
    "            ocr_text = next((ocr[\"ocr_text\"] for ocr in pdf_ocr if ocr[\"page_num\"] == page_num), \"\")\n",
    "            df_data.append({\"file\": file_name, \"text\": page[\"text\"], \"ocr_text\": ocr_text})\n",
    "\n",
    "    elif file_name.endswith(\".pptx\"):\n",
    "        print(f\"üìä Extraction PPTX : {file_name}\")\n",
    "        ppt_text = extract_text_from_pptx(file_path)\n",
    "        for slide in ppt_text:\n",
    "            df_data.append({\"file\": file_name, \"text\": slide[\"text\"], \"ocr_text\": \"\"})\n",
    "\n",
    "# V√©rifier si des donn√©es ont √©t√© extraites\n",
    "if df_data:\n",
    "    df = pd.DataFrame(df_data)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ Extraction termin√©e ! Donn√©es enregistr√©es dans {output_csv}\")\n",
    "else:\n",
    "    print(\"‚ùå Aucune donn√©e extraite, le fichier CSV ne sera pas g√©n√©r√©.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad25fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nettoyage des donn√©es\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Charger le dataset extrait\n",
    "df = pd.read_csv(\"extracted_dataset.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Nettoyage des colonnes \"text\" et \"ocr_text\"\n",
    "def clean_text(text):\n",
    "    # Conversion en minuscules\n",
    "    text = text.lower()\n",
    "\n",
    "    #Suppression des espaces superflus\n",
    "    text = text.strip()\n",
    "\n",
    "    #Suppression des caract√®res sp√©ciaux et des retours √† la ligne inutiles\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Remplacer les sauts de ligne par des espaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Supprimer tous les caract√®res sp√©ciaux\n",
    "\n",
    "    #Suppression des multiples espaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "#Appliquer le nettoyage √† la colonne \"text\" et \"ocr_text\"\n",
    "df['text'] = df['text'].apply(lambda x: clean_text(str(x)))\n",
    "df['ocr_text'] = df['ocr_text'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "#Suppression des doublons (lignes identiques)\n",
    "df = df.drop_duplicates(subset=[\"text\", \"ocr_text\"])\n",
    "\n",
    "#Suppression des lignes avec des valeurs manquantes\n",
    "df = df.dropna(subset=[\"text\", \"ocr_text\"])\n",
    "\n",
    "#V√©rification apr√®s nettoyage\n",
    "print(f\"Data cleaned. Number of rows after cleaning: {len(df)}\")\n",
    "\n",
    "#Sauvegarder les donn√©es nettoy√©es dans un nouveau fichier CSV\n",
    "df.to_csv(\"cleaned_dataset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"Cleaned dataset saved to cleaned_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a81cAg6GjM",
   "metadata": {
    "id": "70a81cAg6GjM"
   },
   "source": [
    "# Chunking method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b450645-fcb1-473f-97f4-1c10096ab437",
   "metadata": {
    "id": "8b450645-fcb1-473f-97f4-1c10096ab437"
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uNCzqqFM6L8j",
   "metadata": {
    "id": "uNCzqqFM6L8j"
   },
   "source": [
    "# chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5d119e6-a5b7-469f-b75c-f29c51ebc2a5",
   "metadata": {
    "id": "d5d119e6-a5b7-469f-b75c-f29c51ebc2a5"
   },
   "outputs": [],
   "source": [
    "chunks_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e0802-9693-45a2-8c6b-a9fef3a183e3",
   "metadata": {
    "id": "8f9e0802-9693-45a2-8c6b-a9fef3a183e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs/Inputs GenAI BMS/Concept MIL High Level Testing.pdf\n",
      "inputs/Inputs GenAI BMS/GrundlagenElektrotechnik_35006.pdf\n",
      "inputs/Inputs GenAI BMS/How2CANalyzer.pdf\n",
      "inputs/Inputs GenAI BMS/How2Controldesk2_Gen5.pdf\n",
      "inputs/Inputs GenAI BMS/How2Ediabas.pdf\n",
      "inputs/Inputs GenAI BMS/How2INCA.pdf\n",
      "inputs/Inputs GenAI BMS/How2JIRA.pdf\n",
      "inputs/Inputs GenAI BMS/Short_ISTQB.pdf\n",
      "inputs/Inputs GenAI BMS/BMS Doc/140228_Ladestrategie_und_Regelungstechnik_Schulung.pdf\n",
      "inputs/Inputs GenAI BMS/BMS Doc/applsci-12-10756-v3.pdf\n",
      "inputs/Inputs GenAI BMS/BMS Doc/Arrow-Infineon-Battery-Management-Systems-BMS Whitepaper.pdf\n",
      "inputs/Inputs GenAI BMS/BMS Doc/Fit4HV_Speicher_Version_2021.pdf\n",
      "inputs/Inputs GenAI BMS/BMS Doc/Infineon-INF1197_ART_BMS_Whitepaper_d08-Whitepaper-v01_00-EN.pdf\n",
      "inputs/Inputs GenAI BMS/BMS Doc/sustainability-14-15912.pdf\n",
      "inputs/ISO 26262 methods/ISO-26262-1.pdf\n",
      "inputs/ISO 26262 methods/ISO-26262-10.pdf\n",
      "MuPDF error: syntax error: unknown keyword: 'l673.99'\n",
      "\n",
      "inputs/ISO 26262 methods/ISO-26262-2.pdf\n",
      "inputs/ISO 26262 methods/ISO-26262-3.pdf\n",
      "inputs/ISO 26262 methods/ISO-26262-4.pdf\n",
      "inputs/ISO 26262 methods/ISO-26262-5.pdf\n",
      "inputs/ISO 26262 methods/ISO-26262-6.pdf\n",
      "inputs/ISO 26262 methods/ISO-26262-7.pdf\n",
      "inputs/ISO 26262 methods/ISO-26262-8.pdf\n",
      "inputs/ISO 26262 methods/ISO-26262-9.pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "def split_dataset_into_chunks(df, chunk_size=800, chunk_overlap=200):\n",
    "    \"\"\"Divise le dataset en chunks en utilisant CharacterTextSplitter.\n",
    "\n",
    "    Args:\n",
    "        df: Le DataFrame Pandas contenant les donn√©es √† diviser.\n",
    "        chunk_size: La taille maximale de chaque chunk en caract√®res.\n",
    "        chunk_overlap: Le chevauchement entre les chunks en caract√®res.\n",
    "\n",
    "    Returns:\n",
    "        Une liste de dictionnaires, o√π chaque dictionnaire repr√©sente un chunk\n",
    "        et contient les cl√©s \"chunk\", \"file\" et \"type\".\n",
    "    \"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    chunks_all = []\n",
    "    for index, row in df.iterrows():\n",
    "        chunks = text_splitter.split_text(row['text'])\n",
    "        for chunk in chunks:\n",
    "            chunks_all.append({\n",
    "                'chunk': chunk,\n",
    "                'file': row['file'],\n",
    "                'type': 'text'  # Indique que le chunk provient de la colonne 'text'\n",
    "            })\n",
    "\n",
    "        # Faire de m√™me pour la colonne 'ocr_text' si n√©cessaire\n",
    "        chunks = text_splitter.split_text(row['ocr_text'])\n",
    "        for chunk in chunks:\n",
    "            chunks_all.append({\n",
    "                'chunk': chunk,\n",
    "                'file': row['file'],\n",
    "                'type': 'ocr_text'  # Indique que le chunk provient de la colonne 'ocr_text'\n",
    "            })\n",
    "    \n",
    "    return chunks_all\n",
    "\n",
    "# Utilisation de la fonction :\n",
    "chunks_all = split_dataset_into_chunks(df) \n",
    "\n",
    "# Enregistrement des chunks dans un fichier CSV si vous le souhaitez :\n",
    "import pandas as pd\n",
    "chunks_df = pd.DataFrame(chunks_all)\n",
    "chunks_df.to_csv(\"chunks_dataset.csv\", index=False, encoding=\"utf-8\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eKdJjB5c5W2-",
   "metadata": {
    "id": "eKdJjB5c5W2-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1118"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411905a5-ef9a-4ea7-8927-d1faffbc01e5",
   "metadata": {
    "id": "411905a5-ef9a-4ea7-8927-d1faffbc01e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Charger le dataset\n",
    "df = pd.read_csv(\"chunks_dataset.csv\")\n",
    "\n",
    "# Fonction pour traduire la colonne\n",
    "def translate_column(column):\n",
    "    translated_column = []\n",
    "    for x in column:\n",
    "        if pd.notna(x):\n",
    "            # Si le texte est trop long, le d√©couper en morceaux plus petits\n",
    "            max_length = 800  # Longueur maximale autoris√©e\n",
    "            chunks = [x[i:i+max_length] for i in range(0, len(x), max_length)]\n",
    "\n",
    "            translated_text = \"\"\n",
    "            for chunk in chunks:\n",
    "                translated_text += GoogleTranslator(source='auto', target='en').translate(chunk) + \" \"\n",
    "            translated_column.append(translated_text.strip())\n",
    "        else:\n",
    "            translated_column.append(x)\n",
    "    return translated_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67de0f8-7ed1-43bd-b248-c1ede30f564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "model_kwargs = {\"trust_remote_code\": True, \"device\": \"cpu\"}\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311cf7d-df98-4294-b47c-0afc22041564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Client\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bdad66-0bd6-44f9-8ba0-fce8bfdcb4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Define the embedding function\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Create a folder to store the database\n",
    "DB_FOLDER = f'db_3'\n",
    "print(DB_FOLDER)\n",
    "\n",
    "# Convert the chunks_all dictionary into a list of Documents\n",
    "documents = []\n",
    "for chunk in chunks_all:\n",
    "    documents.append(Document(page_content=chunk['chunk'], metadata={'file': chunk['file'], 'type': chunk['type']}))\n",
    "\n",
    "# Initialize Chroma with documents and embeddings\n",
    "db = Chroma.from_documents(documents, embedding_function, persist_directory=DB_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c568564a-e795-4cd9-91e6-e8fbe557aae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96d88c-707d-440a-b45d-fc0daabdae9e",
   "metadata": {
    "id": "4f96d88c-707d-440a-b45d-fc0daabdae9e",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1b74e-5358-45e0-a7c3-7ffd1dcb7add",
   "metadata": {
    "id": "cdf1b74e-5358-45e0-a7c3-7ffd1dcb7add",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "-fWFZ8-b6RSX",
   "metadata": {
    "id": "-fWFZ8-b6RSX"
   },
   "source": [
    "# embedding and loading into the main vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56e1364-462a-4adf-a39a-566e8ba2718c",
   "metadata": {
    "id": "a56e1364-462a-4adf-a39a-566e8ba2718c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khbenkha\\Documents\\BMS_chatbot\\env\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262a8779cd7042a78f01dbbf0f4c3628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khbenkha\\Documents\\BMS_chatbot\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\khbenkha\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ab069966e54f7386cf5e5040acc3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36d9bc266664f28873e0030a178f927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4199aa2086194fb9977eba6a07d8631c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa42287be8f54cee8585a3bc9d1979fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503ce126908c4f5fb14468dd5c2d03f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a218ca030fa046e791e4481c1cdb2c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a6d29fcd52451e97039f6e6d80c337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4650329329f64758b75278e98b04610f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khbenkha\\Documents\\BMS_chatbot\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422ae6180ad64958b93db18d55fcf6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "model_kwargs = {\"trust_remote_code\": True, \"device\": \"cpu\"}\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e85bc7ce-b4a2-46a2-af06-af60a8740df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f486c2d5-b33f-4b18-b122-60e47237a109",
   "metadata": {
    "id": "f486c2d5-b33f-4b18-b122-60e47237a109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db_3\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "db_n = db_n + 1\n",
    "DB_FOLDER = f'db_3'\n",
    "print(DB_FOLDER)\n",
    "db = Chroma.from_documents(chunks_all, embedding_function, persist_directory=DB_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drtxTp1G-oIp",
   "metadata": {
    "id": "drtxTp1G-oIp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O_ViN6BH-oLO",
   "metadata": {
    "id": "O_ViN6BH-oLO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
